{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which video is this?\n",
    "vid=24\n",
    "# Number of speakers?\n",
    "speakers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for mp4 files --> .wav files\n",
    "import subprocess\n",
    "command = f\"ffmpeg -i C:\\\\Users\\\\User\\\\Desktop\\\\internship\\\\data\\\\VOA_{vid}\\\\vid_full.mp4 -ab 160k -ac 1 -ar 44100 -vn data/VOA_{vid}/audio.wav\"\n",
    "subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mp3 files --> .wav files\n",
    "import subprocess\n",
    "command = f'ffmpeg -i C:\\\\Users\\\\User\\\\Desktop\\\\internship\\\\VOA_audio\\\\mp3\\\\{vid}_mp3.mp3 -ar 16000 -ac 1 -acodec pcm_s16le C:\\\\Users\\\\User\\\\Desktop\\\\internship\\\\VOA_audio\\\\wav\\\\{vid}_wav.wav'\n",
    "subprocess.call(command, shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sklearn.cluster\n",
    "from pyAudioAnalysis.MidTermFeatures import mid_feature_extraction as mT\n",
    "from pyAudioAnalysis.audioBasicIO import read_audio_file, stereo_to_mono\n",
    "from pyAudioAnalysis.audioSegmentation import labels_to_segments\n",
    "from pyAudioAnalysis.audioTrainTest import normalize_features\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wavfile\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code chunk will import the .wav file, use k-means clustering to generate segments with speaker tags, and then write \n",
    "# the segments and tags to txt file that will pop up. The format will be \"start,end,tag\" for each line of the txt file.\n",
    "\n",
    "# read signal and get normalized segment feature statistics:\n",
    "input_file = f\"VOA_audio/wav/{vid}_wav.wav\" \n",
    "fs, x = read_audio_file(input_file)\n",
    "mt_size, mt_step, st_win = 2, 0.1, 0.05\n",
    "[mt_feats, st_feats, _] = mT(x, fs, mt_size * fs, mt_step * fs, round(fs * st_win), round(fs * st_win * 0.5))\n",
    "(mt_feats_norm, MEAN, STD) = normalize_features([mt_feats.T])\n",
    "mt_feats_norm = mt_feats_norm[0].T\n",
    "\n",
    "# perform clustering\n",
    "n_clusters = speakers\n",
    "x_clusters = [np.zeros((fs, )) for i in range(n_clusters)]\n",
    "k_means = sklearn.cluster.KMeans(n_clusters=n_clusters)\n",
    "k_means.fit(mt_feats_norm.T)\n",
    "cls = k_means.labels_\n",
    "\n",
    "# create segments and classes\n",
    "segs, c = labels_to_segments(cls, mt_step)  # convert flags to segment limits\n",
    "\n",
    "#make segs and tags ready for mach_segs\\\\{vid}.txt file\n",
    "seg2txt = []\n",
    "for i in np.arange(0,len(segs)):\n",
    "    seg2txt.append(str(segs[i][0])+\",\"+ str(segs[i][1])+\",\"+str(c[i]))\n",
    "with open(f'VOA_audio\\\\mach_segs\\\\{vid}.txt', 'w') as f:\n",
    "    for line in seg2txt:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "\n",
    "import webbrowser\n",
    "webbrowser.open(f'VOA_audio\\\\mach_segs\\\\{vid}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os  \n",
    "#os.mkdir(f'data\\\\VOA_{vid}\\\\unsup_dir')\n",
    "\n",
    "num_segs = np.arange(0,len(segs))\n",
    "for i in num_segs:\n",
    "    cur_x = x[int(segs[i, 0] * fs): int(segs[i, 1] * fs)] \n",
    "    print(f'speaker{c[i]}')\n",
    "    print(f'{segs[i,0]} {segs[i,1]}')\n",
    "    wavfile.write(f'data/VOA_{vid}/unsup_dir/clip{num_segs[i]}.wav', fs, np.int16(cur_x)) \n",
    "    IPython.display.display(IPython.display.Audio(f'data/VOA_{vid}/unsup_dir/clip{num_segs[i]}.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "#os.mkdir(f'VOA_audio\\\\unsup_dir')\n",
    "#os.mkdir(f'VOA_audio\\\\unsup_dir\\\\{vid}')\n",
    "\n",
    "num_segs = np.arange(0,len(segs))\n",
    "for i in num_segs:\n",
    "    cur_x = x[int(segs[i, 0] * fs): int(segs[i, 1] * fs)] \n",
    "    print(f'speaker{c[i]}')\n",
    "    print(f'{segs[i,0]} {segs[i,1]}')\n",
    "    wavfile.write(f'VOA_audio\\\\unsup_dir\\\\{vid}\\\\{vid}_{num_segs[i]}.wav', fs, np.int16(cur_x)) \n",
    "    IPython.display.display(IPython.display.Audio(f'VOA_audio\\\\unsup_dir\\\\{vid}\\\\{vid}_{num_segs[i]}.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make directory\n",
    "os.mkdir(f'VOA_audio/aud_clips')\n",
    "#make subdirectories\n",
    "for i in np.arange(1,53):\n",
    "    os.mkdir(f'VOA_audio/aud_clips/{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1,53):\n",
    "    input_file = f\"VOA_audio/wav/{i}_wav.wav\" \n",
    "    fs, x = read_audio_file(input_file)\n",
    "    # input segments and classes\n",
    "    segs_test = np.genfromtxt(f'VOA_audio/clean_segs/{i}.txt',delimiter=',')\n",
    "    segs_test_s = []\n",
    "    segs_test_c=[]\n",
    "    for c in segs_test:\n",
    "        segs_test_s.append([c[0],c[1]])\n",
    "        segs_test_c.append(int(c[2]))\n",
    "    segs = np.array(segs_test_s)\n",
    "    cl = np.array(segs_test_c)\n",
    "    \n",
    "    #Write segments to audio files\n",
    "    num_segs = np.arange(0,len(segs))\n",
    "    for n in num_segs:\n",
    "        cur_x = x[int(segs[n, 0] * fs): int(segs[n, 1] * fs)]    \n",
    "        wavfile.write(f'VOA_audio/aud_clips/{i}/clip{num_segs[n]}_{cl[n]}.wav', fs, np.int16(cur_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(f'VOA_audio\\\\clean{vid}_segs.txt', 'w') as f:\n",
    "    for line in seg_str:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from pyAudioAnalysis.audioBasicIO import read_audio_file\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wavfile\n",
    "import IPython\n",
    "import os\n",
    "\n",
    "input_file = f\"data/VOA_{vid}/audio.wav\" \n",
    "fs, x = read_audio_file(input_file)\n",
    "# input segments and classes\n",
    "segs_test = np.genfromtxt(f'data\\\\VOA_{vid}\\\\clean_segs.txt',delimiter=',')\n",
    "segs_test_s = []\n",
    "segs_test_c=[]\n",
    "for i in segs_test:\n",
    "    segs_test_s.append([i[0],i[1]])\n",
    "    segs_test_c.append(int(i[2]))\n",
    "segs = np.array(segs_test_s)\n",
    "cl = np.array(segs_test_c)\n",
    "\n",
    "#make directory\n",
    "os.mkdir(f'data\\\\VOA_{vid}\\\\clip_dir')\n",
    "\n",
    "#Write segments to audio files\n",
    "num_segs = np.arange(0,len(segs))\n",
    "for i in num_segs:\n",
    "    cur_x = x[int(segs[i, 0] * fs): int(segs[i, 1] * fs)] \n",
    "    print(f'speaker{cl[i]}')\n",
    "    print(f'clip_number_{i}')\n",
    "    print(f'{segs[i,0]} {segs[i,1]}')\n",
    "    wavfile.write(f'data/VOA_{vid}/clip_dir/clip{num_segs[i]}_{cl[i]}.wav', fs, np.int16(cur_x))\n",
    "    IPython.display.display(IPython.display.Audio(f'data/VOA_{vid}/clip_dir/clip{num_segs[i]}_{cl[i]}.wav'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example 3: segment-level feature extraction\n",
    "from pyAudioAnalysis import MidTermFeatures as aF\n",
    "from pyAudioAnalysis import audioBasicIO as aIO \n",
    "\n",
    "# read audio data from file \n",
    "# (returns sampling freq and signal as a numpy array)\n",
    "fs, s = aIO.read_audio_file(\"data/VOA_1/anna_pete_mixed.wav\")\n",
    "\n",
    "# get mid-term (segment) feature statistics \n",
    "# and respective short-term features:\n",
    "mt, st, mt_n = aF.mid_feature_extraction(s, fs, 1 * fs, 1 * fs, \n",
    "                                         0.05 * fs, 0.05 * fs)\n",
    "print(f'signal duration {len(s)/fs} seconds')\n",
    "print(f'{st.shape[1]} {st.shape[0]}-D short-term feature vectors extracted')\n",
    "print(f'{mt.shape[1]} {mt.shape[0]}-D segment feature statistic vectors extracted')\n",
    "print('mid-term feature names')\n",
    "for i, mi in enumerate(mt_n):\n",
    "    print(f'{i}:{mi}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example6: use pyAudioAnalysis wrapper \n",
    "# to extract feature and train SVM classifier \n",
    "# for 2 speakers (2 anna_matteo/2 pete) audio samples\n",
    "\n",
    "from pyAudioAnalysis.audioTrainTest import extract_features_and_train\n",
    "mt, st = 2.25, 0.05\n",
    "dirs = [\"data/VOA_1/train/male_short\", \"data/VOA_1/train/anna_short\"] \n",
    "extract_features_and_train(dirs, mt, mt, st, st, \"svm_rbf\", \"anna_notanna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyAudioAnalysis import audioTrainTest as aT\n",
    "files_to_test = [\"data/VOA_1/train/male_short/1.wav\",\n",
    "                 \"data/VOA_1/train/anna_short/1.wav\",\n",
    "                 \"data/VOA_1/train/anna_short/2.wav\"]\n",
    "for f in files_to_test:\n",
    "    print(f'{f}:')\n",
    "    c, p, p_nam = aT.file_classification(f, \"anna_notanna\",\"svm_rbf\")\n",
    "    print(f'P({p_nam[0]}={p[0]})')\n",
    "    print(f'P({p_nam[1]}={p[1]})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example 11\n",
    "# Supervised audio segmentation example:\n",
    "#  - Apply model \"anna_notanna\" to achieve fix-sized, supervised audio segmentation \n",
    "#    on file data/VOA_1/audio.wav\n",
    "#  - Function audioSegmentation.mid_term_file_classification() uses pretrained model and applies \n",
    "#    the mid-term step that has been used when training the model (1 sec in our case as shown in Example6)\n",
    "#  - data/VOA_1/lab_segs.segments contains the ground truth of the audio file\n",
    "\n",
    "from pyAudioAnalysis.audioSegmentation import mid_term_file_classification, labels_to_segments\n",
    "from pyAudioAnalysis.audioTrainTest import load_model\n",
    "labels, class_names, _, _ = mid_term_file_classification(\"data/VOA_1/anna_pete_mixed.wav\", # audio.wav \n",
    "                                                         \"anna_notanna\", \"svm_rbf\",  True, \n",
    "                                                         \"data/VOA_1/lab_segs.segments\")  \n",
    "print(\"\\nFix-sized segments:\")\n",
    "for il, l in enumerate(labels):\n",
    "    print(f'fix-sized segment {il}: {class_names[int(l)]}')\n",
    "\n",
    "# load the parameters of the model (actually we just want the mt_step here):    \n",
    "cl, m, s, m_classes, mt_win, mt_step, s_win, s_step, c_beat = load_model(\"anna_notanna\")\n",
    "\n",
    "# print \"merged\" segments (use labels_to_segments())\n",
    "print(\"\\nSegments:\")\n",
    "segs, c = labels_to_segments(labels, mt_step)\n",
    "for iS, seg in enumerate(segs):\n",
    "    print(f'segment {iS} {seg[0]} sec - {seg[1]} sec: {class_names[int(c[iS])]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intshp",
   "language": "python",
   "name": "intshp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
